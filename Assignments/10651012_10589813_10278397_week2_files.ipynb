{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Notebook made by  \n",
    "\n",
    "|** Name** | **Student id** | **email**|\n",
    "|:- |:-|:-|\n",
    "|Dylan Wijman |10651012|dylanwijman@hotmail.com|\n",
    "|Eline Steensma|10589813|elinesteensma@gmail.com|\n",
    "|Sjoerd Paardekooper|10278397|sjoerd.paardekooper@gmail.com|\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. \n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='https://github.com/dylanwijman/data-science/blob/master/Afbeeldingen/selfies.jpg?raw=true'/>\n",
    "\n",
    "\n",
    "### Note\n",
    "* **Assignments without the selfies or completely filled in information will not be graded and receive 0 points.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: obtaining information from the web\n",
    "\n",
    "### RSS parsing\n",
    "\n",
    "Make a notebook that performs the following steps.\n",
    "\n",
    "1. Create a script that retrieves all urls of rss feeds from <http://www.volkskrant.nl/rss/feeds/>. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
    "    * **update 2016**\n",
    "    * As all Dutch sites, Volkskrant asks whether you accept cookies. This makes simple collecting webpages a lot harder. \n",
    "    * The code in the code cell below does the trick. \n",
    "    * After running this, I could collect further files from Volkskrant without additional cookie hassle.\n",
    "2. Download all rss feeds and store them on disk.\n",
    "3. Parse all RSS feeds using `lxml`. Create a list of  dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
    "4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
    "5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
    "6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
    "7. Count per day the number of words, and the number of unique words. Show this in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cookielib # Thanks to http://stackoverflow.com/questions/29395407/enabling-cookies-with-urllib\n",
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import feedparser\n",
    "from lxml import etree\n",
    "from lxml import objectify\n",
    "from itertools import chain\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "url = 'https://rawgit.com/maartensukel/Data-Science/master/deVolkskrant.html'\n",
    "\n",
    "# with urllib2 and handling cookies\n",
    "cookiejar= cookielib.LWPCookieJar()\n",
    "opener= urllib2.build_opener( urllib2.HTTPCookieProcessor(cookiejar) )\n",
    "response=opener.open(url)\n",
    "html_doc= ' '.join(response.readlines())\n",
    " \n",
    "rsssoup = BeautifulSoup(html_doc, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Maak een lijst met alle urls van de RSSfeed van de volkskrant\n",
    "feeds = [link.get('href') for link in rsssoup.findAll(href=re.compile(\"\\.xml$\"))]\n",
    "rssDataPerChannel = []\n",
    "rssData = []\n",
    "tags = [\"channel\", \"url\", \"title\", \"date\"]\n",
    "# Loop door de RSS feed links en sla de xml lokaal op\n",
    "for link in feeds:\n",
    "    response=opener.open(link)\n",
    "    xml_doc= ' '.join(response.readlines())\n",
    "    xmlsoup = BeautifulSoup(xml_doc, \"lxml\")\n",
    "    name = xmlsoup.title.string+'.html'\n",
    "    #with open(name, 'w') as f:\n",
    "        #f.write(xmlsoup.encode())\n",
    "    #f.close()\n",
    "    channel = xmlsoup.title.string\n",
    "    items = xmlsoup.findAll('item')\n",
    "    for item in items:\n",
    "        title = item.title\n",
    "        link = item.findAll(text=re.compile('http://'))[0]\n",
    "        date = item.pubdate.string\n",
    "        itemData = {'channel': channel, 'title': title, 'date': date, 'link': link}\n",
    "        rssData.append(itemData)\n",
    "    rssDataPerChannel.append({tag: xmlsoup.find_all(tag) for tag in tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 9, 40, 40, 0, 0, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "b: 1169\n",
      "c: 36\n",
      "348\n"
     ]
    }
   ],
   "source": [
    "# 1.4\n",
    "# a. Lijst met het aantal berichten per RSS feed\n",
    "lenItemsPerFeed = [len(channel.find_all('item')) for channel in [feed['channel'][0] for feed in rssDataPerChannel]]\n",
    "print \"a:\", lenItemsPerFeed\n",
    "# b. Amount of items per feed\n",
    "print \"b:\", sum(lenItemsPerFeed)\n",
    "# c. Average amount of items\n",
    "print \"c:\", sum(lenItemsPerFeed)/len(lenItemsPerFeed)\n",
    "# d. Zijn er dubbels?\n",
    "allTitles = [item['title'] for item in rssData]\n",
    "print \"d:\", sum([count-1 for title, count in Counter(allTitles).items() if count > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.5\n",
    "with open('eenpuntvijf.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(rssData[1].keys())\n",
    "    for row in rssData:\n",
    "        writer.writerow(row.values())\n",
    "        \n",
    "# link naar Google fusions table: https://www.google.com/fusiontables/DataSource?docid=1mp3ooLyQhJfg8tq6qqr6SuOVIPW3wenikEgC-NN_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JSON parsing\n",
    "\n",
    "1. Download <http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb>, remove all code blocks, and turn it into a notebook again. \n",
    "2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
    "3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import notebook\n",
    "url = \"http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb\"\n",
    "jsonfile= urllib2.urlopen(url)\n",
    "jsonfile = jsonfile.read()  # The jsonfile as a Python string\n",
    "json_as_python_object = json.loads(jsonfile) # The josnfile transformed into a Python dict\n",
    "cells = json_as_python_object['worksheets'][0]['cells']\n",
    "codeblocks = [cell['input'] for cell in cells if str(cell['cell_type']) == 'code']\n",
    "f = open(\"output.py\", \"w\")\n",
    "for x in codeblocks:\n",
    "    for y in x:\n",
    "      f.write(str(y))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PDF parsing\n",
    "\n",
    "1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use _difficult_ PDF. Why not try <http://wch.github.io/latexsheet/latexsheet.pdf>?\n",
    "\n",
    "* Is the word order still as it should be?\n",
    "* What about the strange characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl http://wch.github.io/latexsheet/latexsheet.pdf > latex.pdf; pdftotext  latex.pdf\n",
    "!pdftotext latex.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl http://www.koopman-assurantien.nl/images1/%28118%29%2008.08.pdf > koopman.pdf; pdftotext koopman.pdf\n",
    "!pdftotext koopman.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl https://github.com/dylanwijman/data-science/blob/master/Documenten/grondwet-voor-het-koningrijk-der-nederlanden-1815.pdf > grondwet.pdf; pdftotext grondwet.pdf\n",
    "!pdftotext grondwet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word document\n",
    "https://github.com/dylanwijman/data-science/blob/master/Documenten/grondwet-voor-het-koningrijk-der-nederlanden-1815.doc\n",
    "\n",
    "PDF document\n",
    "https://github.com/dylanwijman/data-science/blob/master/Documenten/grondwet-voor-het-koningrijk-der-nederlanden-1815.pdf\n",
    "\n",
    "#### 1. Verschillen tussen .doc en .pdf text extracting files\n",
    "De verschillen tussen de originele .doc en het geparste pdf bestand zijn niet heel groot. De tekst staat slechts in 1 kolom waardoor het niet lastig is om te parsen en er geen opmaak verloren gaat.\n",
    "\n",
    "#### 1.2 Try the same with a two column PDF file from the web.\n",
    "\n",
    "Bij de 'Koopman' PDF file met twee kolommen kan de file worden extract zonder het behouden van de layout (-layout). Op deze manier wordt er niet in de leesrichting gezocht naar tekens, van links naar rechts. Op deze manier worden de kolommen tekst bijeen gehouden. Enkele koppen in het 'Koopman' bestand zijn wel verkeerd geplaatst, in plaats van onder elkaar wordt de leesrichting hier nog wel aangehouden als koppen op dezelfde horizontale regel staan.\n",
    "\n",
    "This exercise gets more interesting if you use difficult PDF\n",
    "#### 2 Is the word order still as it should be?\n",
    "\n",
    "In het Latex bestand worden de kolommen in kolommen niet goed onderscheiden als tekst die naast elkaar staat, ook niet met -layout. De leesvolgorde gaat na Packages naar Lists en daarna ineens naar de derde kolom naar Justification, waaronder alle informatie van Lists staat.\n",
    "\n",
    "#### 3 What about the strange characters?\n",
    "\n",
    "De accenten zijn niet allemaal goed weergeven en vaak uit elkaar gehaald. Een Ç (c cedille) wordt niet als 1 teken weergeven maar een losse , (komma) en en c: , c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2 Python recap\n",
    "\n",
    "Download [PythonRecap2.0.ipynb](PythonRecap2.0.ipynb),  and answer all questions as asked."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
