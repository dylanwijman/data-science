{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Notebook made by  \n",
    "\n",
    "|** Name** | **Student id** | **email**|\n",
    "|:- |:-|:-|\n",
    "|Dylan Wijman |10651012|dylanwijman@hotmail.com|\n",
    "|Eline Steensma|10589813|elinesteensma@gmail.com|\n",
    "|Sjoerd Paardekooper|10278397|sjoerd.paardekooper@gmail.com|\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. \n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='https://github.com/dylanwijman/data-science/blob/master/Afbeeldingen/selfies.jpg?raw=true'/>\n",
    "\n",
    "\n",
    "### Note\n",
    "* **Assignments without the selfies or completely filled in information will not be graded and receive 0 points.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: obtaining information from the web\n",
    "\n",
    "### RSS parsing\n",
    "\n",
    "Make a notebook that performs the following steps.\n",
    "\n",
    "1. Create a script that retrieves all urls of rss feeds from <http://www.volkskrant.nl/rss/feeds/>. Use urllib2 and beautifulsoup for this. Store the urls in a list.\n",
    "    * **update 2016**\n",
    "    * As all Dutch sites, Volkskrant asks whether you accept cookies. This makes simple collecting webpages a lot harder. \n",
    "    * The code in the code cell below does the trick. \n",
    "    * After running this, I could collect further files from Volkskrant without additional cookie hassle.\n",
    "2. Download all rss feeds and store them on disk.\n",
    "3. Parse all RSS feeds using `lxml`. Create a list of  dicts with fields \"channel\", \"url\", \"title\", \"date\" in which you store this information for each item.\n",
    "4. Compute some statistics about this dict: how many items, how many per channel, are there doubles (items occuring in several channels), etc.\n",
    "5. Write this list as a csv file, store on disk, and upload to Google fusion tables.\n",
    "6. Download all articles (once), parse out the text and store as pairs (date,text) in a list.\n",
    "7. Count per day the number of words, and the number of unique words. Show this in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dylan/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import cookielib # Thanks to http://stackoverflow.com/questions/29395407/enabling-cookies-with-urllib\n",
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import feedparser\n",
    "from lxml import etree\n",
    "from lxml import objectify\n",
    "\n",
    "url = 'http://www.volkskrant.nl/rss/feeds/'\n",
    "\n",
    "# with urllib2 and handling cookies\n",
    "cookiejar= cookielib.LWPCookieJar()\n",
    "opener= urllib2.build_opener( urllib2.HTTPCookieProcessor(cookiejar) )\n",
    "response=opener.open(url)\n",
    "html_doc= ' '.join(response.readlines())\n",
    " \n",
    "rsssoup = BeautifulSoup(html_doc)\n",
    "\n",
    "# test \n",
    "# list_items=rsssoup.findAll('li')\n",
    "# len(list_items), list_items[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 9, 40, 40, 0, 0, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "b: 1169\n",
      "c: 36\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'items' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fbdb95d32293>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mitemsPerFeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'item'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchannel\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'channel'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrssData\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Maak 1 lijst met alle items\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitemsPerFeed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;31m# Geeft lijst met een count van items die meer dan een keer voorkomen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# http://stackoverflow.com/questions/9835762/find-and-list-duplicates-in-python-list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'items' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Maak een lijst met alle urls van de RSSfeed van de volkskrant\n",
    "feeds = [link.get('href') for link in rsssoup.findAll(href=re.compile(\"\\.xml$\"))]\n",
    "\n",
    "rssData = []\n",
    "tags = [\"channel\", \"url\", \"title\", \"date\"]\n",
    "# Loop door de RSS feed links en sla de xml lokaal op\n",
    "for link in feeds:\n",
    "    response=opener.open(link)\n",
    "    xml_doc= ' '.join(response.readlines())\n",
    "    xmlsoup = BeautifulSoup(xml_doc, \"lxml\")\n",
    "    name = xmlsoup.title.string+'.html'\n",
    "    #with open(name, 'w') as f:\n",
    "        #f.write(xmlsoup.encode())\n",
    "    #f.close()\n",
    "    # Maak een dictionary met keys: channel, url, title en date aan per link in de feed\n",
    "    # Dict comprehension voor 1.3\n",
    "    rssData.append({tag: xmlsoup.find_all(tag) for tag in tags})\n",
    "    \n",
    "# 1.4\n",
    "# Channel is een CSS tag: <channel>. Komt een keer voor per feed vandaar.\n",
    "# a. Lijst met het aantal berichten per RSS feed\n",
    "lenItemsPerFeed = [len(channel.find_all('item')) for channel in [feed['channel'][0] for feed in rssData]]\n",
    "print \"a:\", lenItemsPerFeed\n",
    "# b. Amount of items per feed\n",
    "print \"b:\", sum(lenItemsPerFeed)\n",
    "# c. Average amount of items\n",
    "print \"c:\", sum(lenItemsPerFeed)/len(lenItemsPerFeed)\n",
    "# d. Zijn er dubbels?\n",
    "itemsPerFeed = [channel.find_all('item') for channel in [feed['channel'][0] for feed in rssData]]\n",
    "# Maak 1 lijst met alle items\n",
    "items = list(chain.from_iterable(itemsPerFeed))\n",
    "# Geeft lijst met een count van items die meer dan een keer voorkomen en len geeft het aantal items weer die dubbel voorkomen\n",
    "# http://stackoverflow.com/questions/9835762/find-and-list-duplicates-in-python-list\n",
    "# Hieronder is gecomment door lange wachttijd\n",
    "print \"d:\", len([count for item, count in Counter(items).items() if count > 1])\n",
    "\n",
    "# 1.5\n",
    "\n",
    "with open('eenpuntvijf.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JSON parsing\n",
    "\n",
    "1. Download <http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb>, remove all code blocks, and turn it into a notebook again. \n",
    "2. Check that what you did is correct and you did not remove too much using a notebook viewer.\n",
    "3. Now extract all code from the downloaded notebook, save it to a file, and execute it as a Python script. Does it give errors? Is it syntactically correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import notebook\n",
    "url = \"http://maartenmarx.nl/teaching/ISatWork/NoteBooks/consuming-json-data-from-a-web-service.ipynb\"\n",
    "jsonfile= urllib2.urlopen(url)\n",
    "jsonfile = jsonfile.read()  # The jsonfile as a Python string\n",
    "json_as_python_object = json.loads(jsonfile) # The josnfile transformed into a Python dict\n",
    "cells = json_as_python_object['worksheets'][0]['cells']\n",
    "codeblocks = [cell['input'] for cell in cells if str(cell['cell_type']) == 'code']\n",
    "f = open(\"output.py\", \"w\")\n",
    "for x in codeblocks:\n",
    "    for y in x:\n",
    "      f.write(str(y))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PDF parsing\n",
    "\n",
    "1. Save a wordfile as PDF, open it in Python, extract all text. Describe the differences, if any. Try the same with a two column PDF file from the web. This exercise gets more interesting if you use _difficult_ PDF. Why not try <http://wch.github.io/latexsheet/latexsheet.pdf>?\n",
    "\n",
    "* Is the word order still as it should be?\n",
    "* What about the strange characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  292k  100  292k    0     0   480k      0 --:--:-- --:--:-- --:--:--  504k\n"
     ]
    }
   ],
   "source": [
    "!curl http://wch.github.io/latexsheet/latexsheet.pdf > latex.pdf; pdftotext  latex.pdf\n",
    "!pdftotext latex.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  480k  100  480k    0     0  1064k      0 --:--:-- --:--:-- --:--:-- 1146k\n"
     ]
    }
   ],
   "source": [
    "!curl http://www.koopman-assurantien.nl/images1/%28118%29%2008.08.pdf > koopman.pdf; pdftotext koopman.pdf\n",
    "!pdftotext koopman.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2 Python recap\n",
    "\n",
    "Download [PythonRecap2.0.ipynb](PythonRecap2.0.ipynb),  and answer all questions as asked."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
